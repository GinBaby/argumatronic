---
title: A Brief Guide to A Few Algebraic Structures
tags: math, reference
withtoc: yes
---

### Intro

I started writing this post because, for whatever reason, I keep forgetting what the difference is between a *ring* and a *group*, which is funny to me because I never forget the difference between a *semiring* and a *semigroup* -- although other people do, because it's quite easy to forget! So, I wanted a fast reference to the kinds of algebraic structures that I am most often dealing with in one way or another, usually because I'm writing Haskell (which has some reliance on terminology and structure from abstract algebra and category theory) or I'm trying to read a book about category theory and they keep talking about "groups." Wikipedia, of course, defines all these structures, and that's fine, but what I need in those times is more of a refresher than an in-depth explanation. 

So, that's the intent of this post -- to be that fast reference to the kinds of algebraic structures I care about. There will be some mathematical language and notation that may not be familiar to everyone, so don't freak out if you see something you're not familiar with -- it's okay. Some of this is stuff I learned while writing this, because I want to know more about lattices than I currently do! There is a glossary at the end, so if some terminology is unfamiliar or you've (as I have, many times) forgotten what "idempotent" means, try checking there.

This sort of became longer than I originally intended, but I think the different sections might be useful at different times and perhaps to different audiences, so I kept it all. Hopefully the table of contents will still make it useful as a quick reference.

I had some help writing this and fact-checking it from [Daniel Brice](https://twitter.com/fried_brice) -- therefore, I will stop talking about myself as if I wrote it alone and shift to using "we" but, sorry, it felt weird to write this intro as if I were  multiple people. Our hope is that this reference will be helpful to people who may be learning Haskell or PureScript and encountering a lot of new vocabulary.


# Algebraic structures, briefly described

This list is organized by dependencies, more or less, rather than alphabetically. The focus is on the subsets of the group-like, ring-like, and lattice-like structures, but is not all-inclusive. We may add to this as time goes on and new structures grab our fancy.

Some symbols used in this section:

- $\ast$ and $\times$: A generic binary operation. These do not necessarily mean *multiplication (as of integers)*.  
- $u$: An identity element or unit.  
- $'$: An inverse.

For definitions of terms, see [glossary](#a-little-glossary).


## Group-like Structures

### *Magma*

A set with a (closed) binary operation.
 
 $(A, \ast)$
 
 **Structure:**
 
 * $\ast : A \times A \to A$

### *Semigroup*

A magma where the operation is associative.

$(A, \ast)$

**Structure:**

* $\ast : A \times A \to A$

**Laws:**

* $\forall x, y, z \in A; x \ast (y \ast z) = (x \ast y) \ast z$

### *Monoid* 

A semigroup with an identity element.

$(A, \ast, u)$

**Structure:**

* $\ast : A \times A \to A$
* $u : A$

**Laws:**

* $(A, \ast)$ is a semigroup
* $\forall x \in A; x \ast u = u \ast x = x$

### *Group* 

A monoid that has inverses relative to the operation.

$(A, \ast, u, ')$

**Structure:**

* $\ast : A \times A \to A$
* $u : A$
* $' : A \to A$

**Laws:**

* $(A, \ast, u)$ is a monoid
* $\forall x \in A; x \ast x' = x' \ast x = u$

### *Abelian group*

A group where the operation is also commutative. You may also see the term *abelian* applied to semigroups and monoids whose operations are commutative.

Semigroup $(A, \ast)$

**Laws:**

* $\forall x, y \in A; x \ast y = y \ast x$

This is in addition to the laws for semigroup, monoid, or group (whichever abelian structure you're dealing with) that already pertain.

### *Idempotent group*

A group whose operation is idempotent. Strictly speaking, an idempotent group is necessarily trivial -- that is, it is necessarily a group with only one element. As with *abelian*, *idempotent* may apply to semigroups or monoids as well when the operation is idempotent. 

Semigroup $(A, \ast)$

**Laws:**

* $\forall x \in A; x \ast x = x$


## Ring-like Structures

### *Quasiring*

Two monoids over the same set whose monoid structures are compatible, in the sense that
 
 1. one operation (called multiplication) distributes over the other (called addition) and  
 2. the additive identity is a multiplicative annihilator.

$(A, +, \cdot, 0, 1)$

**Structure:**

* $+ : A \times A \to A$
* $\cdot : A \times A \to A$
* $0 : A$
* $1 : A$

**Laws:**

* $(A, +, 0)$ is a monoid
* $(A, \cdot, 1)$ is a monoid
* $\forall x \in A; 0 \cdot x = x \cdot 0 = 0$
* $\forall x, y, z \in A; x \cdot (y+z) = x \cdot y + x \cdot z$
* $\forall x, y, z \in A; (x+y) \cdot z = x \cdot z + y \cdot z$


### *Nearring*

A quasiring with additive inverses.

$(A, +, \cdot, 0, 1, -)$

**Structure:**

* $+ : A \times A \to A$
* $\cdot : A \times A \to A$
* $0 : A$
* $1 : A$
* $- : A \to A$

**Laws:**

* $(A, +, 0, -)$ is a group

### *Semiring*

A quasiring with commutative addition. Alternatively, a ring without inverses, hence also sometimes called a *rig*, i.e., a ring without *n*egatives.

Quasiring $(A, +, \cdot, 0, 1)$

**Laws:**

* $(A, +, 0)$ is abelian

### *Rng* 

A ring without *i*dentities. 

If we may speak frankly, the *rig* and *rng* nomenclatures are abominations. Nevertheless, you may see them sometimes, but we will speak of them no more.

### *Ring* 

A quasiring that is both a nearring and a semiring. Alternatively, Abelian group plus a monoid (over the same set) where the monoid operation is distributive over the group operation.

Nearring $(A, +, \cdot, 0, 1, -)$

**Laws:**

* $(A, +, 0, -)$ is an abelian group

Rings, semirings, and the lot can also be commutative rings, commutative semirings, etc., but, unlike the group-like structures, they are not usually described as, e.g., "abelian rings."

### *Division Algebra*

A ring with multiplicative inverses.

$(A, +, \cdot, 0, 1, -, ')$

**Structure:**

* $+ : A \times A \to A$
* $\cdot : A \times A \to A$
* $0 : A$
* $1 : A$
* $- : A \to A$
* $' : A \setminus \{0\} \to A \setminus \{0\}$

**Laws:**

* $(A, +, \cdot, 0, 1, -)$ is a ring
* $(A \setminus \{0\}, 1, ')$ is a group


### *Field*

A division algebra with commutative multiplication.

Division algebra $(A, +, \cdot, 0, 1, -, ')$

**Laws:**

* $(A, +, \cdot, 0, 1)$ is commutative


## Lattice-like Structures

### *Semilattice* 

A magma where the operation is commutative, associative, and idempotent. It could refer to either of the abelian semigroups of a lattice, often called $\vee$ *join* or Boolean *or*, and $\wedge$ *meet* or Boolean *and*. We define the term here for reasons related to its usage in Haskell, but it is best understood in context of lattices, rather than independently. 

### *Lattice* 

Two idempotent abelian semigroups over the same set whose semigroup structures are compatible, in the sense that the operations satisfy absorption laws. Interestingly, it's sort of *two semilattices* in the same way that a semiring is *two monoids*, with laws tying them together (distributivity in the case of semirings, absorption laws in the case of lattices).

$(A, \vee, \wedge)$

**Structure:**

* $\vee : A \times A \to A$
* $\wedge : A \times A \to A$

**Laws:**

* $(A, \vee)$ is an idempotent abelian semigroup
* $(A, \wedge)$ is an idempotent abelian semigroup
* $\forall x, y \in A; x \vee (x \wedge y) = x$
* $\forall x, y \in A; x \wedge (x \vee y) = x$
<!-- wait is this right -- for all x in A? i would have thought there exists an x in A such that ... ?

The last law is the *absorption* law, a special case of identity. Since absorption laws are unique to lattices, we discuss them here instead of in the glossary. The absorption laws link a pair of binary operations by linking the identity of one operation, as zero is the identity of addition, to its role as the annihilator of the other operation (multiplication).
Absorption, on the other hand, refers specifically to something more like a distributive relationship between the two binary operations in lattice-like algebraic structures.

In a Boolean lattice, that element is false. Consider this, where `||` corresponds to $\vee$ and `&&` corresponds to $\wedge$:

```haskell
λ> False || (False && True)
False

λ> False && (False || True)
False
```




### *Bounded Lattice*

$(A, \vee, \wedge, 0, 1)$

**Summary:**

A _bounded lattice_ is a lattice whose semigroup structures are monoids.

**Structure:**

* $\vee : A \times A \to A$
* $\wedge : A \times A \to A$
* $0 : A$
* $1 : A$

**Laws:**

* $(A, \vee, \wedge)$ is a lattice
* $(A, \vee, 0)$ is a monoid
* $(A, \wedge, 1)$ is a monoid


* Complemented Lattice

$(A, \vee, \wedge, 0, 1, ')$

**Summary:**

A _complemented lattice_ is a bounded lattice where each element has a compliment. (BEWARE: Although $'$ defines a particular choice of complements [i.e., each element $x \in A$ has exactly one corresponding $x' \in A$], there may additionally be other elements $y \in A$ such that $x \vee y = 1$ and $x \wedge y = 0$. In particular, there may be other suitable $'$ functions, and $x''$ is not necessarily $x$.)

**Structure:**

* $\vee : A \times A \to A$
* $\wedge : A \times A \to A$
* $0 : A$
* $1 : A$
* $' : A \to A$

**Laws:**

* $(A, \vee, \wedge, 0, 1)$ is a bounded lattice
* $\forall x \in A; x \vee x' = 1$
* $\forall x \in A; x \wedge x' = 0$


* Distributive Lattice

Lattice $(A, \vee, \wedge)$

**Summary:**

A _distributive lattice_ is a lattice where the operations distribute over each other. (Strictly speaking, the second law can be derived from the first law and the lattice laws, and as such is redundant.)

**Laws:**

* $\forall x, y, z \in A; x \wedge (y \vee z) = (x \wedge y) \vee (x \wedge z)$
* $\forall x, y, z \in A; x \vee (y \wedge z) = (x \vee y) \wedge (x \vee z)$


* Heyting Algebra

$(A, \vee, \wedge, 0, 1, \Rightarrow)$

**Summary:**

A _Heyting algebra_ is a bounded lattice with an implication operation.

**Structure:**

* $\vee : A \times A \to A$
* $\wedge : A \times A \to A$
* $0 : A$
* $1 : A$
* $\Rightarrow : A \times A \to A$

**Laws:**

* $\forall x \in A; a \Rightarrow a = 1$
* $\forall x, y \in A; a \wedge (a \Rightarrow b) = a \wedge b$
* $\forall x, y \in A; b \wedge (a \Rightarrow b) = b$
* $\forall a, b, c \in A; a \Rightarrow (b \wedge c) = (a \Rightarrow b) \wedge (a \Rightarrow c)$


* Boolean Algebra

Complemented lattice $(A, \vee, \wedge, 0, 1, ')$

**Summary:**

A _boolean algebra_ is a complemented Heyting algebra.

**Laws:**

Let $\Rightarrow : A \times A \to A$ by $x \Rightarrow y = x' \vee y$.

* $(A, \vee, \wedge, 0, 1, \Rightarrow)$ is a Heyting algebra




# Some algebras in Haskell

The typeclass system of Haskell more or less corresponds to algebraic structures, with types as the sets. The typeclass definition gives the most general possible form of the operations over the sets, and the `instance` declarations define the implementations of those operations for the specified type (set). This is how algebra became relevant to my adult life.

Not all of the above algebras are well represented in Haskell, but some are, and we give those typeclass definitions, a representative `instance` or two, and links to documentation where appropriate.

One important thing to note before we get started that is, perhaps, somewhat disappointing: the compiler does not enforce the laws of the algebraic typeclasses. The only thing standing between you and a law-breaking ring or monoid is ... well ... you, and your willingness to test your instances, I suppose.


### Semigroup

The [`Semigroup`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Semigroup.html) class in Haskell is defined as follows:

```haskell
class Semigroup a where
  -- | An associative operation.
  (<>) :: a -> a -> a
```

Many sets form semigroups under more than one operation, so in Haskell, to preserve the unique relationship between a type and a typeclass, we use a named type wrapper, called a `newtype`, which identifies which semigroup we're talking about.
<!-- This is precisely the same situation in Math. A set can't be a group (or a ring, etc). Instead, a group has to be a pair (G, *) where `G` is a set and `* : (G, G) -> G` and the group axioms hold. `G` is not the group: the group is the pairing of the set `G` with the structure function `*`. In Haskell, we use `newtypes` to pair a set (in this case, the type being wrapped) with some structure functions. -->

```haskell
-- the Max semigroup is only for orderable sets
instance Ord a => Semigroup (Max a) where
  (<>) = coerce (max :: a -> a -> a)

-- the NonEmpty semigroup is concatenation of nonempty lists
instance Semigroup (NonEmpty a) where
  (a :| as) <> ~(b :| bs) = a :| (as ++ b : bs)
```

I've written extensively about [`Semigroup` over on Type Classes](https://typeclasses.com/semigroup).

### Monoid

In modern Haskell, `Semigroup` is a superclass of [`Monoid`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Monoid.html). That is, since monoids are semigroups with the additional requirement that there be an identity element, semigroup is in some sense the weaker algebra and there are more of them than there are monoids. What this means is if we want a `Monoid`, we have to first have a `Semigroup`; the binary operation comes from the `Semigroup` instance.
<!-- This is one of the most annoying things about Haskell, and the fix is purely syntactical: allow a subclass instance declaration to provide implementations of superclass methods if there's not a separate instance declaration of the superclass. This could even fix the problem in Haskell where you can't slide in new type classes between existing type classes. And it's purely a syntax change. But no, we have to write out half a dozen instance declarations and constantly break backwards compatibility. -->
Then we define the identity element for that type and operation in our `Monoid` instance -- in the `Monoid` class it's called `mempty`.

```haskell
class Semigroup a => Monoid a where
  -- | Identity of '<>'
  mempty  :: a
```

Again, many sets form monoids under more than one operation, so we use `newtype`s in Haskell to tell them apart.

```haskell
instance Num a => Semigroup (Sum a) where
  (<>) = coerce ((+) :: a -> a -> a)

instance Num a => Monoid (Sum a) where
  mempty = Sum 0

instance Num a => Semigroup (Product a) where
  (<>) = coerce ((*) :: a -> a -> a)

instance Num a => Monoid (Product a) where
  mempty = Product 1
```

I have also written extensively about [`Monoid` over on Type Classes](https://typeclasses.com/monoid), about [JavaScript and monoidal folds](https://typeclasses.com/javascript/monoidal-folds), and also [given talks](/posts/2017-10-12-haskellx17-talk.html) about these wonderful structures.

It is perhaps worth pointing out that the [`Alternative`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Applicative.html#g:2) and [`MonadPlus`](http://hackage.haskell.org/package/monadplus-1.4.2/docs/Control-Monad-Plus.html) typeclasses in Haskell are *also* monoids. The difference between them and the `Monoid` class is that `Monoid` is a typeclass for concrete types, whereas `Alternative` and `MonadPlus` are for type constructors, that is, parameterized types.
<!-- We should probably make this more precise. Something like this:
  > If `f` is an `Alternative`, then for all `a`, `f a` is a monoid under `<|>`, with identity `empty`. We encode this fact in Haskell via the `Alt` newtype and its `Monoid` instance.
Notice I intentionally did not capitalize "monoid" above, because `(Alternative f) => f a` doesn't have a `Monoid` instance, so it's not a `Monoid`, but it certainly is a monoid (under `<|>`), whether or not it has a `Monoid`. -->

### Ring

We don't exactly have a `Ring` typeclass in standard Haskell; what we have instead is the `Num` class and it's sort of like a ring. It's a big typeclass, so this is a simplified version with the functions you'd expect a ring to have.
<!-- While `abs` and `signum` have nothing at all to do with rings, perhaps surprisingly `fromInteger` has everything to do with rings and rightly belongs in the definition of any type class for rings. This is because the ring of integers is an initial object in the category of rings, i.e. for every ring `A`, there is always one and only one ring homomorphism (e.g. ring-structure-compatible function) from the ring of integers to the ring `A`. This is probably beyond the scope of the blog post, but I just thought you might things it's fun/interesting to know. -->

```haskell
class  Num a  where
  (+), (-), (*)       :: a -> a -> a
  negate              :: a -> a
```

[Comments in the source code](http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Num.html) say:

> The Haskell Report defines no laws for 'Num'. However, '(+)' and '(*)` are customarily expected to define a ring

and then give the properties a ring is expected to have; however, those laws are not enforced in any way, and I suspect many people do not even think of `Num` as a ring that should have laws. Let us speak no more of this.


### Semiring

It's a shame that `Semiring` is not in the standard library (yet?). It is in the [standard PureScript library](https://pursuit.purescript.org/packages/purescript-prelude/3.1.0/docs/Data.Semiring), and I really admire PureScript for that, among other things. However, we have some decent implementations of it in libraries. My preference at this time is [this `semirings` package](http://hackage.haskell.org/package/semirings).

A `Semiring` definition looks like this:

```haskell
class Semiring a where
  plus  :: a -> a -> a -- commutative operation
  zero  :: a           -- identity for `plus`
  times :: a -> a -> a -- associative operation
  one   :: a           -- identity for `times`
```

And a Boolean semiring -- the `instance` of that typeclass for `Bool` -- could be implemented thusly:

```haskell
instance Semiring Bool where
  plus  = (||)
  zero  = False
  times = (&&)
  one   = True
```

<!-- This is perhaps incorrect. There's some ambiguity around the definition of a boolean semiring, so it's best not to bring them up, I think. There is a well-established notion of a boolean ring, and in a boolean ring, `plus x y` is defined as `(x && not y) || (y && not x)`, not as `x || y` (thought the other methods are still defined as you have them above). -->

I have also written more about [semirings on Type Classes](https://typeclasses.com/semiring).

### Lattice and semilattice

I have a growing interest in these structures but have not yet written much about them or used the [`lattices`](https://hackage.haskell.org/package/lattices-1.4) package. I notice that that package defines two semilattice classes and then a `Lattice` class that is constrained by *both*. We note that the `Lattice` class has no new functions in it; you can use it as a constraint on other things when you have two semilattices (the meet and the join) and the absorption law holds.

```haskell
class JoinSemiLattice a where
    (\/) :: a -> a -> a

class MeetSemiLattice a where
    (/\) :: a -> a -> a

class (JoinSemiLattice a, MeetSemiLattice a) => Lattice a where
```

And the instances for `Bool` are defined:

```haskell
instance JoinSemiLattice Bool where
    (\/) = (||)

instance MeetSemiLattice Bool where
    (/\) = (&&)

instance Lattice Bool where
```

The absorption law does hold for the `Bool` lattice, so I guess we're all good here!

<!-- I really admire the work you put in to making the subject matter both approachable and correct. Too often people tend to sacrifice one for the sacrifice of the other, but I don't think it's as zero-sum as they think it is: I think things can be both. I'm glad you do, too. -->

# A little glossary

This first section gives definitions of some common terminology when talking about the laws and properties of these structures.

Some of these will probably be familiar to most people from high school math, others may not be.

- **Absorption** : See [lattices](#lattice-like-structures).

- **Annihilator** : We have to include this term because it is such a metal thing to call zeroes. An annihilating element is an element of a set that always absorbs the other element, sort of the opposite of an identity element (see below). If `(S, *)` is a set `S` with a binary operation `*` on it, the annihilator, or zero element, is an element `z` such that for all `a` in `S`, `z * a = a * z = z`. In the monoid of integer multiplication, the annihilator is zero, while in the monoid of Boolean conjunction, the annihilator is `False`.

- **Associativity** : The associative property is a property of some binary operations. In expressions containing two or more successive applications of the same associative operator, the order in which the operations are performed does not affect the result. That is, repeated application of the operation produces the same result regardless of how we might insert parentheses to group the subexpressions. More formally, an operation `*` on a set `S` is *associative* when for all `x`, `y`, and `z` in `S`, `x * (y * z) = (x * y) * z`.

- **Closed** : By definition, a binary operation over a set implies that the operation is *closed*, that is, for all `a`, `b`, in set `S`, the result of the binary operation `a * b` is also an element in `S`.
<!-- ^ This coincides exactly with the definition of a function `(S, S) -> S`, maybe that's worth mentioning? Rather, instead of defining "closed", define "binary operations." Something like "A *binary operation* `*` on a set `S` is a function `* : (S, S) -> S`. Notice that `*` maps `(S, S)` back into `S`. Because of an historical quirk, this fact is sometimes called *closure*." -->
In Haskell, that looks like a type signature such as `a -> a -> a`.
<!-- ^ Delicious curry -->
Also, sometimes called the property of *closure*.
<!-- ^ Historically, the idea of requiring "closure" as an axiom of abstract algebraic structures predates the modern notion of functions. They required "closure" because they didn't have the modern vocabulary of functions, domains, and codomains. Since the vocabulary of functions, domains, and codomains is so flexible and precise, we should probably dispense with mentioning "closure" as a property of algebraic structures entirely, mentioning it as a historical quirk in the definition of "binary operation" exactly once, then immediately proceeding in the more-suitable framing of ideas that speaking in terms of functions, domains, and codomains allows. -->

- **Commutativity** : Commutativity is not the same as associativity, although most commutative operations are also associative.
The commutative property of some binary operations holds that changing the order of the inputs does not affect the result.
More formally, an operation `*` on a set `S` is *commutative* when for all `x` and `y` in `S`, `x * y = y * x`.
Commutativity is also closely related to *symmetry*.
<!-- ^ A "binary relation" `R` on `S` is a function `R : (S, S) -> Bool`. We'd use the term *symmetric* to refer to certain binary relations, but not really to refer to binary operations, so I don't know if mentioning "symmetry" adds to clarity; it might even add to confusion. -->

- **Distributivity** : The distributive property in arithmetic states that multiplication distributes over addition such that `2 * (1 + 3) = (2 * 1) + (2 * 3)`.
<!-- I like how you lead with a familiar example. Maybe the other definitions can also lead off with an example that will be familiar to the read. E.g. for associative:
  > Recall from arithmetic that (2 * 9) * 5 and 2 * (9 * 5) must always give the same result, no matter which pair you decide to multiply first. When the result never depends on the order of simplification, we say that a binary operation is *associative.* Formally, for all ... ... ... . Importantly, not all binary operations are associative. (For example, is `-` associative?)
-->
In algebra, this is generalized such that for any set `S` with two binary operators, `*` and `+`, `*` *distributes over* `+` when for all `x`, `y`, and `z` in `S`, `x * (y + z) = (x * y) + (y * z)` (left distributive) *and* `(y + z) * x = (y * x) + (z * x)` (right distributive).
<!-- The wording "In algebra, this is generalized such that for any set `S` with two binary operators..." doesn't sit right with me. It almost sounds like an assertion that any structure with two operations must satisfy the distributive law. Can we try instead something more like this:
  > Some algebraic structures generalize this with their own distributive law. Suppose we have a set `S` with two binary operations, `<>` and `><`. We say `><` *distributes over* `<>` when for all... -->
Note that if `*` is commutative and left distributive, it follows that it is also right distributive (and therefore distributive).

- **Idempotence** : Idempotence is a property of some binary operations under which applying the operation multiple times doesn't change the result after the first application.
Consider a device where there are separate buttons for turning the device on and off; pushing the *on* button doesn't turn it "more on", so the *on* button is idempotent (and so is the *off* button).
<!-- In these first two sentences, you're defining idempotence for unary functions, not for binary operations. For instance, the effect of pressing the "on" button is a unary function: it takes a state and returns a state (with state space { on, off}). It's a good example, but of a different, related concept. Let's not worry about defining idempotence for unary functions and instead jump straight to the definition of idempotent elements of a binary operation instead. -->
We say an element of a set is idempotent with respect to some operation `*` if `x * x = x`.
It seems that both absorption and identity elements should always be idempotent elements.
<!-- Replace "absorption" with "annihilator" above :-) -->
We say an operation is idempotent if every element in the set is idempotent with respect to the operation.
For the natural numbers under multiplication, both `1` and `0` are idempotent; for the naturals under addition, only `0` is.
Hence neither addition nor multiplication of the natural numbers is itself idempotent.
However, taking the absolute value of integers is an idempotent function.
<!-- Replace "operation" with "function" above. Operation isn't technically wrong, but we're trying to stick to using the word "operation" for binary operations. -->
Furthermore, the set operations of union and intersection are both idempotent operations.
<!-- Excellent examples. -->

- **Identity** : An identity element is an element of a set that is neutral with respect to some binary operation on that set; that is, it leaves any other element of that set unchanged when combined with it.
An identity value is unique with respect to the given set and operation.
Most identity values take the shape of something analogous to `1` (products) or `0` (sums).
<!-- Always great to provide the reader with examples grounded in prior experience. Though I'm not sure I know what's meant by "take the shape of something analogous to" means in this context. I think we should keep the examples but find a better wording. -->
More formally, for a set `S` with a binary operation `*` on it, `e` is the identity value when `e * a = a * e = a` for all `a` in `S`.

- **Invertibility** : This is a property of groups.
<!-- None of the other definitions mention the sorts of structures they feature in. Also, inverses is also a property in fields. Together, those observations make me feel like this first sentence is sort of out of place. -->
For each `a` in the set `G`, there exists an element `b` in `G` such that `ab = ba = e` where `e` is the identity element for the group.
<!-- I like how you defined idempotent elements first, and then defined idempotent binary operations. Maybe we can do something similar here. Maybe something like this:
  > Given a binary operation `*` on `S` with identity `e`, an element `b` in `S` is said to be an *inverse* of an element `a` in `S` if `a * b = e = b * a`, in which case `a` (as well as `b`, simply by the symmetry in the definition) is said to be *invertible* in `S` relative to `*`. If every element of `S` is invertible in `S` relative to `*`, then we say `S` has inverses relative to `*`. -->
In the case of addition, then, we'd say `a + b = b + a = 0`.
This is also sometimes referred to as the property of having inverses or negatives (e.g., the negative integers).

## Left and right

You may sometimes hear about *left* or *right* associativity or identity. For example, exponentiation is only *right-associative*. That is, in a chain of such operations, they group for evaluation purposes from the right.

```haskell
λ> 2^3^2
512
-- is the same as
λ> 2^(3^2)
512
-- but not
λ> (2^3)^2
64
```

This is more of a convention than a property of the function, though, and it is often preferable to use parentheses to make associativity explicit when it is one-sided (i.e., when something is right associative but not *associative*). We call something *associative* when it is both left- and right-associative. We call something *distributive* when it is both left- and right-distributive. We call something an *identity* if it is both a left and right identity.

